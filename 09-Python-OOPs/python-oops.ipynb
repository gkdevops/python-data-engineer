{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Classes and Objects\n",
    "\n",
    "A **class** is a blueprint for creating objects. It defines attributes (data) and methods (functions) that the objects will have.\n",
    "\n",
    "## Why Use Classes?\n",
    "- Organize code into reusable structures\n",
    "- Model real-world entities\n",
    "- Bundle data and functionality together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining a Class\n",
    "\n",
    "Use the `class` keyword to define a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple DataSource class\n",
    "class DataSource:\n",
    "    pass\n",
    "\n",
    "# Create an object of the class\n",
    "source1 = DataSource()\n",
    "print(source1)\n",
    "print(type(source1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Attributes\n",
    "\n",
    "Attributes are variables that belong to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseConfig:\n",
    "    # Class attributes - shared across all instances\n",
    "    default_port = 5432\n",
    "    default_schema = \"public\"\n",
    "    timeout_seconds = 30\n",
    "\n",
    "# Create config objects\n",
    "postgres_config = DatabaseConfig()\n",
    "redshift_config = DatabaseConfig()\n",
    "\n",
    "# Access class attributes\n",
    "print(f\"Default Port: {postgres_config.default_port}\")\n",
    "print(f\"Default Schema: {postgres_config.default_schema}\")\n",
    "print(f\"Timeout: {redshift_config.timeout_seconds}s\")\n",
    "\n",
    "# Modify attributes for specific instance\n",
    "redshift_config.default_port = 5439\n",
    "redshift_config.default_schema = \"analytics\"\n",
    "\n",
    "print(f\"\\nPostgres Port: {postgres_config.default_port}\")\n",
    "print(f\"Redshift Port: {redshift_config.default_port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The `__init__()` Method (Constructor)\n",
    "\n",
    "The `__init__()` method is called automatically when an object is created. It's used to initialize the object's attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseConnection:\n",
    "    # Constructor\n",
    "    def __init__(self, host, database, user, password, port=5432):\n",
    "        self.host = host\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.port = port\n",
    "        self.is_connected = False\n",
    "\n",
    "# Create connection objects with initial values\n",
    "prod_db = DatabaseConnection(\n",
    "    host=\"prod-db.company.com\",\n",
    "    database=\"analytics\",\n",
    "    user=\"etl_user\",\n",
    "    password=\"secret123\",\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "staging_db = DatabaseConnection(\n",
    "    host=\"staging-db.company.com\",\n",
    "    database=\"staging\",\n",
    "    user=\"dev_user\",\n",
    "    password=\"dev123\"\n",
    ")\n",
    "\n",
    "print(f\"Production DB: {prod_db.host}:{prod_db.port}/{prod_db.database}\")\n",
    "print(f\"Staging DB: {staging_db.host}:{staging_db.port}/{staging_db.database}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Methods\n",
    "\n",
    "Methods are functions defined inside a class that describe the behaviors of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, source_name, source_type):\n",
    "        self.source_name = source_name\n",
    "        self.source_type = source_type\n",
    "        self.records_extracted = 0\n",
    "        self.last_extraction = None\n",
    "    \n",
    "    # Instance method\n",
    "    def extract(self, query):\n",
    "        \"\"\"Simulate data extraction\"\"\"\n",
    "        print(f\"Extracting from {self.source_name} ({self.source_type})...\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Simulated data\n",
    "        data = [\n",
    "            {\"id\": 1, \"name\": \"Alice\", \"amount\": 100.50},\n",
    "            {\"id\": 2, \"name\": \"Bob\", \"amount\": 250.75},\n",
    "            {\"id\": 3, \"name\": \"Charlie\", \"amount\": 180.25}\n",
    "        ]\n",
    "        \n",
    "        self.records_extracted += len(data)\n",
    "        self.last_extraction = datetime.now()\n",
    "        print(f\"Extracted {len(data)} records\")\n",
    "        return data\n",
    "    \n",
    "    # Method to get extraction stats\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"source\": self.source_name,\n",
    "            \"type\": self.source_type,\n",
    "            \"total_records\": self.records_extracted,\n",
    "            \"last_run\": str(self.last_extraction)\n",
    "        }\n",
    "\n",
    "# Create extractor and use methods\n",
    "sales_extractor = DataExtractor(\"sales_db\", \"PostgreSQL\")\n",
    "data = sales_extractor.extract(\"SELECT * FROM transactions\")\n",
    "\n",
    "print(f\"\\nExtraction Stats:\")\n",
    "print(json.dumps(sales_extractor.get_stats(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Example: ETL Pipeline Class\n",
    "\n",
    "Let's create a more complete example with an ETL Pipeline class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "class ETLPipeline:\n",
    "    # Class attribute\n",
    "    supported_formats = [\"csv\", \"json\", \"parquet\"]\n",
    "    \n",
    "    def __init__(self, pipeline_name, source_path, target_path):\n",
    "        # Instance attributes\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.source_path = source_path\n",
    "        self.target_path = target_path\n",
    "        self.status = \"initialized\"\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.records_processed = 0\n",
    "        self.errors = []\n",
    "    \n",
    "    def extract(self):\n",
    "        \"\"\"Extract data from source\"\"\"\n",
    "        print(f\"[EXTRACT] Reading from: {self.source_path}\")\n",
    "        self.status = \"extracting\"\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "        # Simulated raw data\n",
    "        raw_data = [\n",
    "            {\"customer_id\": \"C001\", \"order_date\": \"2024-01-15\", \"amount\": \"150.50\", \"status\": \"completed\"},\n",
    "            {\"customer_id\": \"C002\", \"order_date\": \"2024-01-16\", \"amount\": \"invalid\", \"status\": \"pending\"},\n",
    "            {\"customer_id\": \"C003\", \"order_date\": \"2024-01-17\", \"amount\": \"275.00\", \"status\": \"completed\"},\n",
    "            {\"customer_id\": None, \"order_date\": \"2024-01-18\", \"amount\": \"100.00\", \"status\": \"completed\"},\n",
    "        ]\n",
    "        print(f\"[EXTRACT] Found {len(raw_data)} records\")\n",
    "        return raw_data\n",
    "    \n",
    "    def transform(self, raw_data):\n",
    "        \"\"\"Transform and clean data\"\"\"\n",
    "        print(f\"\\n[TRANSFORM] Processing {len(raw_data)} records...\")\n",
    "        self.status = \"transforming\"\n",
    "        \n",
    "        transformed_data = []\n",
    "        for record in raw_data:\n",
    "            try:\n",
    "                # Skip records with null customer_id\n",
    "                if record[\"customer_id\"] is None:\n",
    "                    self.errors.append({\"record\": record, \"error\": \"NULL customer_id\"})\n",
    "                    continue\n",
    "                \n",
    "                # Parse and validate amount\n",
    "                try:\n",
    "                    amount = float(record[\"amount\"])\n",
    "                except ValueError:\n",
    "                    self.errors.append({\"record\": record, \"error\": \"Invalid amount\"})\n",
    "                    continue\n",
    "                \n",
    "                # Create transformed record\n",
    "                transformed_record = {\n",
    "                    \"customer_id\": record[\"customer_id\"],\n",
    "                    \"order_date\": datetime.strptime(record[\"order_date\"], \"%Y-%m-%d\"),\n",
    "                    \"amount\": amount,\n",
    "                    \"status\": record[\"status\"].upper(),\n",
    "                    \"processed_at\": datetime.now(),\n",
    "                    \"row_hash\": hashlib.md5(str(record).encode()).hexdigest()[:8]\n",
    "                }\n",
    "                transformed_data.append(transformed_record)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.errors.append({\"record\": record, \"error\": str(e)})\n",
    "        \n",
    "        print(f\"[TRANSFORM] Successfully transformed {len(transformed_data)} records\")\n",
    "        print(f\"[TRANSFORM] Errors: {len(self.errors)}\")\n",
    "        return transformed_data\n",
    "    \n",
    "    def load(self, transformed_data):\n",
    "        \"\"\"Load data to target\"\"\"\n",
    "        print(f\"\\n[LOAD] Writing to: {self.target_path}\")\n",
    "        self.status = \"loading\"\n",
    "        \n",
    "        # Simulate loading\n",
    "        for record in transformed_data:\n",
    "            self.records_processed += 1\n",
    "        \n",
    "        self.status = \"completed\"\n",
    "        self.end_time = datetime.now()\n",
    "        print(f\"[LOAD] Successfully loaded {self.records_processed} records\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute full ETL pipeline\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Starting Pipeline: {self.pipeline_name}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        try:\n",
    "            raw_data = self.extract()\n",
    "            transformed_data = self.transform(raw_data)\n",
    "            self.load(transformed_data)\n",
    "        except Exception as e:\n",
    "            self.status = \"failed\"\n",
    "            self.errors.append({\"error\": str(e)})\n",
    "            print(f\"[ERROR] Pipeline failed: {e}\")\n",
    "        \n",
    "        return self.get_run_summary()\n",
    "    \n",
    "    def get_run_summary(self):\n",
    "        \"\"\"Return pipeline run summary\"\"\"\n",
    "        duration = None\n",
    "        if self.start_time and self.end_time:\n",
    "            duration = (self.end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"pipeline_name\": self.pipeline_name,\n",
    "            \"status\": self.status,\n",
    "            \"records_processed\": self.records_processed,\n",
    "            \"errors_count\": len(self.errors),\n",
    "            \"duration_seconds\": duration,\n",
    "            \"errors\": self.errors\n",
    "        }\n",
    "\n",
    "\n",
    "# Run the ETL Pipeline\n",
    "orders_pipeline = ETLPipeline(\n",
    "    pipeline_name=\"daily_orders_etl\",\n",
    "    source_path=\"s3://raw-data/orders/\",\n",
    "    target_path=\"s3://warehouse/fact_orders/\"\n",
    ")\n",
    "\n",
    "summary = orders_pipeline.run()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Pipeline Summary:\")\n",
    "print(f\"{'='*50}\")\n",
    "for key, value in summary.items():\n",
    "    if key != \"errors\":\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practice Exercise\n",
    "\n",
    "Create a `DataValidator` class with:\n",
    "- Attributes: `dataset_name`, `rules`, `validation_results`\n",
    "- Methods: `add_rule()`, `validate()`, `get_report()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class DataValidator:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.rules = []\n",
    "        self.validation_results = []\n",
    "    \n",
    "    def add_rule(self, column, rule_type, rule_value=None):\n",
    "        \"\"\"Add a validation rule\"\"\"\n",
    "        rule = {\n",
    "            \"column\": column,\n",
    "            \"rule_type\": rule_type,\n",
    "            \"rule_value\": rule_value\n",
    "        }\n",
    "        self.rules.append(rule)\n",
    "        print(f\"Added rule: {rule_type} on column '{column}'\")\n",
    "    \n",
    "    def validate(self, data):\n",
    "        \"\"\"Validate data against all rules\"\"\"\n",
    "        print(f\"\\nValidating {len(data)} records against {len(self.rules)} rules...\")\n",
    "        \n",
    "        for rule in self.rules:\n",
    "            column = rule[\"column\"]\n",
    "            rule_type = rule[\"rule_type\"]\n",
    "            rule_value = rule[\"rule_value\"]\n",
    "            \n",
    "            passed = 0\n",
    "            failed = 0\n",
    "            \n",
    "            for record in data:\n",
    "                value = record.get(column)\n",
    "                \n",
    "                # Check different rule types\n",
    "                if rule_type == \"not_null\":\n",
    "                    if value is not None and value != \"\":\n",
    "                        passed += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        \n",
    "                elif rule_type == \"min_value\":\n",
    "                    if value is not None and float(value) >= rule_value:\n",
    "                        passed += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        \n",
    "                elif rule_type == \"max_length\":\n",
    "                    if value is None or len(str(value)) <= rule_value:\n",
    "                        passed += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        \n",
    "                elif rule_type == \"in_list\":\n",
    "                    if value in rule_value:\n",
    "                        passed += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "            \n",
    "            result = {\n",
    "                \"column\": column,\n",
    "                \"rule_type\": rule_type,\n",
    "                \"passed\": passed,\n",
    "                \"failed\": failed,\n",
    "                \"pass_rate\": round(passed / len(data) * 100, 2)\n",
    "            }\n",
    "            self.validation_results.append(result)\n",
    "        \n",
    "        print(\"Validation complete!\")\n",
    "    \n",
    "    def get_report(self):\n",
    "        \"\"\"Generate validation report\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Data Quality Report: {self.dataset_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        total_passed = 0\n",
    "        total_failed = 0\n",
    "        \n",
    "        for result in self.validation_results:\n",
    "            status = \"PASS\" if result[\"failed\"] == 0 else \"FAIL\"\n",
    "            print(f\"\\n{status} | {result['column']} ({result['rule_type']})\")\n",
    "            print(f\"      Passed: {result['passed']} | Failed: {result['failed']} | Rate: {result['pass_rate']}%\")\n",
    "            total_passed += result[\"passed\"]\n",
    "            total_failed += result[\"failed\"]\n",
    "        \n",
    "        overall_rate = round(total_passed / (total_passed + total_failed) * 100, 2)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Overall Quality Score: {overall_rate}%\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return {\"passed\": total_passed, \"failed\": total_failed, \"score\": overall_rate}\n",
    "\n",
    "\n",
    "# Test the DataValidator\n",
    "sample_data = [\n",
    "    {\"user_id\": \"U001\", \"email\": \"alice@example.com\", \"age\": 28, \"status\": \"active\"},\n",
    "    {\"user_id\": \"U002\", \"email\": None, \"age\": 35, \"status\": \"active\"},\n",
    "    {\"user_id\": \"U003\", \"email\": \"charlie@example.com\", \"age\": -5, \"status\": \"inactive\"},\n",
    "    {\"user_id\": None, \"email\": \"david@example.com\", \"age\": 42, \"status\": \"pending\"},\n",
    "    {\"user_id\": \"U005\", \"email\": \"eve@example.com\", \"age\": 31, \"status\": \"unknown\"},\n",
    "]\n",
    "\n",
    "# Create validator and add rules\n",
    "validator = DataValidator(\"user_profiles\")\n",
    "validator.add_rule(\"user_id\", \"not_null\")\n",
    "validator.add_rule(\"email\", \"not_null\")\n",
    "validator.add_rule(\"age\", \"min_value\", 0)\n",
    "validator.add_rule(\"status\", \"in_list\", [\"active\", \"inactive\", \"pending\"])\n",
    "\n",
    "# Run validation\n",
    "validator.validate(sample_data)\n",
    "\n",
    "# Get report\n",
    "report = validator.get_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Description | Data Engineering Example |\n",
    "|---------|-------------|-------------------------|\n",
    "| `class` | Blueprint for creating objects | `ETLPipeline`, `DataValidator` |\n",
    "| `object` | Instance of a class | `orders_pipeline`, `validator` |\n",
    "| `self` | Reference to the current instance | Access pipeline attributes |\n",
    "| `__init__` | Constructor method | Initialize connection params |\n",
    "| Attributes | Variables belonging to an object | `source_path`, `records_processed` |\n",
    "| Methods | Functions belonging to a class | `extract()`, `transform()`, `load()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Real-World Data Engineering Class Patterns\n",
    "\n",
    "Common classes you'll build as a Data Engineer:\n",
    "- **DatabaseConnection** - Manage database connections\n",
    "- **ETLPipeline** - Orchestrate Extract-Transform-Load jobs\n",
    "- **DataValidator** - Check data quality rules\n",
    "- **SchemaManager** - Handle schema evolution\n",
    "- **FileHandler** - Read/write various file formats\n",
    "- **BatchProcessor** - Process large datasets in chunks\n",
    "- **ConfigManager** - Manage pipeline configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}